{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9139b5-1dad-43c4-b180-85a44f073eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5adfe-5982-477b-9953-625f4c959068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best models\n",
    "import pickle\n",
    "with open('./results/relu_best_models.pickle', 'rb') as best_models_pickle:\n",
    "    best_models = pickle.load(best_models_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722472f2-a6c7-4c9c-9465-6f8d2388385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 5 # max dimention of the input vector\n",
    "n_samples = 10 # number of sampled quadratic function for each dimension\n",
    "\n",
    "times_n = {} # dict that stores solution times\n",
    "objs_n = {} # dict that stores obj function values\n",
    "num_binvars_n = {} # dict that stores # binary variables\n",
    "num_constrs_n = {} # dict that stores # constraints\n",
    "\n",
    "# set random seed\n",
    "global_seed = 777\n",
    "random.seed(global_seed)\n",
    "np.random.seed(global_seed)\n",
    "seeds = np.random.randint(1,1000,(1,n_samples))[0]\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50965ecc-5ca6-48c2-a268-3aef8c1fde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tqdm(range(1, n_max+1)):\n",
    "\n",
    "    times = []\n",
    "    objs = []\n",
    "    num_binvars = []\n",
    "    num_constrs = []\n",
    "\n",
    "    for n_sample in tqdm(range(n_samples)):\n",
    "        \n",
    "        # data reading\n",
    "        random.seed(seeds[n_sample])\n",
    "        np.random.seed(seeds[n_sample])\n",
    "        filename = './' + str(n) + '_' + str(n_sample) + '_train.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Y = ReLU_Network(X)\n",
    "        X_variable_raw = df.iloc[:, 0:-1].values\n",
    "        Y_variable_raw = df.iloc[:, -1].values\n",
    "        \n",
    "        # scaling\n",
    "        sc_X = MinMaxScaler()\n",
    "        sc_y = MinMaxScaler()\n",
    "\n",
    "        X_variable = sc_X.fit_transform(X_variable_raw)\n",
    "        Y_variable = sc_y.fit_transform(Y_variable_raw.reshape(-1,1))\n",
    "        Y_variable = np.squeeze(Y_variable)\n",
    "        \n",
    "        # set random seed\n",
    "        random.seed(global_seed)\n",
    "        np.random.seed(global_seed)\n",
    "        \n",
    "        # use the model trained via quadratic_model_training.ipynb\n",
    "        nn = best_models[n][n_sample]\n",
    "        Y_predict = nn.predict(X_variable)\n",
    "    \n",
    "        # obtain MILP parameters from the ReLU network structure/parameters\n",
    "        # weight and bias\n",
    "        weights = nn.coefs_\n",
    "        biases = nn.intercepts_\n",
    "        \n",
    "        # kj_raw: number of neurons in the ith hidden layer; k_hidden: number of hidden layers\n",
    "        kj_raw = nn.hidden_layer_sizes\n",
    "        k_hidden = nn.n_layers_ - 2\n",
    "        # indices_k_hidden: set of hidden layers; indices_kj_hidden: set of tuple (layer k, jth neuron in this layer)\n",
    "        # indices_kj_dict: mapping of hidden layer k -> list of neurons j in this hidden layer\n",
    "        indices_k_hidden = range(1, k_hidden+1)\n",
    "        indices_kj_hidden = []\n",
    "        indices_kj_dict = defaultdict(list)\n",
    "        for k in indices_k_hidden:\n",
    "            for j in range(kj_raw[k-1]):\n",
    "                indices_kj_hidden.append((k,j))\n",
    "                indices_kj_dict[k].append(j)\n",
    "        indices_kj_dict[nn.n_layers_ - 1] = [0]\n",
    "        indices_kj_dict[0] = list(range(n))\n",
    "        # indices_kj_first: (k, j) tuple for input layer; indices_kj_last: (k, j) tuple for the output layer\n",
    "        # indices_kj: (k, j) tuple for all the layers in the network\n",
    "        # indices_kj_nofirst: (k, j) tuple for all the layers except for the first layer\n",
    "        indices_kj_first = [(0,i) for i in range(n)]\n",
    "        indices_kj_last = [(nn.n_layers_ - 1, 0)]\n",
    "        indices_kj = indices_kj_first + indices_kj_hidden + indices_kj_last\n",
    "        indices_kj_nofirst = indices_kj_hidden + indices_kj_last\n",
    "    \n",
    "        # paramters\n",
    "        \n",
    "        # lower/upper bound of variable X\n",
    "        # x_lb_first/x_ub_first: lower/upper bound of variable X for the input layer\n",
    "        x_lb_first = [-2] * n\n",
    "        x_ub_first = [2] * n\n",
    "        \n",
    "        # x_ub/x_lb: lower/upper bound of variable X in all the layers\n",
    "        # x_ub_dict: mapping of layer -> list of upper bound of all neurons in the layer\n",
    "        bigM = 1e5\n",
    "        \n",
    "        x_ub = x_ub_first + [bigM] * len(indices_kj_hidden) + [bigM]\n",
    "        x_lb = x_lb_first + [0] * len(indices_kj_hidden) + [-bigM]\n",
    "        x_ub_dict = defaultdict(list)\n",
    "        x_ub_dict[0] = x_ub_first\n",
    "        for k in indices_k_hidden:\n",
    "            x_ub_dict[k] = [bigM] * kj_raw[k-1]\n",
    "        x_ub_dict[k_hidden + 1] = [bigM]\n",
    "        \n",
    "        # scaling: the network is trained using the min-max scaled variables, so we need to have scaler_coeff to do\n",
    "        # (1) min_max scaling the input variable (happened at the input layer)\n",
    "        # (2) min_max scaling the output variable (happened at the output layer)\n",
    "        # to make the constraints consistent we still have scaler_coeff[k] for hidden layer but the values are set to 1 (no scaling)\n",
    "        scaler_coeff = {}\n",
    "        scaler_coeff[0] = np.reciprocal(sc_X.data_range_).tolist() # scaler_coeff = 1/(Xmax - Xmin) (i.e., the original range)\n",
    "        for k in indices_k_hidden:\n",
    "            scaler_coeff[k] = [1] * kj_raw[k-1] # scaler_coeff = 1 (no scaling)\n",
    "        scaler_coeff[k_hidden + 1] = np.reciprocal(sc_y.data_range_).tolist() # scaler_coeff = 1/(Ymax - Ymin)\n",
    "        \n",
    "        scaler_min = {}\n",
    "        scaler_min[0] = sc_X.data_min_.tolist() # Xmin before scaling\n",
    "        for k in indices_k_hidden:\n",
    "            scaler_min[k] = [0] * kj_raw[k-1] # Xmin = 0 (no scaling)\n",
    "        scaler_min[k_hidden + 1] = sc_y.data_min_.tolist() # Ymin before scaling\n",
    "    \n",
    "        #create a new model\n",
    "        m = gp.Model(\"RELU\")\n",
    "        m.Params.LogToConsole = 0\n",
    "        m.setParam(GRB.Param.TimeLimit, 1800.0)\n",
    "\n",
    "        #create variables\n",
    "        z = m.addVars(indices_kj_nofirst, name = 'z', vtype = GRB.BINARY)\n",
    "        x = m.addVars(indices_kj, ub = x_ub, lb = x_lb, name = 'x')\n",
    "        s = m.addVars(indices_kj_nofirst, name = 's')\n",
    "    \n",
    "        # Maximization\n",
    "        m.setObjective(x[nn.n_layers_-1,0], GRB.MINIMIZE)\n",
    "\n",
    "        # Add constraint\n",
    "        m.addConstrs((gp.quicksum(weights[k-1][l][j] * (x[k-1,l] - scaler_min[k-1][l]) * scaler_coeff[k-1][l] for l in indices_kj_dict[k-1]) + biases[k-1][j] \\\n",
    "                      == (x[k,j] - scaler_min[k][j]) * scaler_coeff[k][j] - s[k,j] for (k,j) in indices_kj_nofirst),\n",
    "                     name = 'calc_layers')\n",
    "        m.addConstrs((x[k,j] <= x_ub_dict[k][j] * z[k,j] for (k,j) in indices_kj_nofirst), name = \"constraint_x\")\n",
    "        m.addConstrs((s[k,j] <= x_ub_dict[k][j] * (1 - z[k,j]) for (k,j) in indices_kj_nofirst), name = \"constraint_z\")\n",
    "    \n",
    "        m.update()\n",
    "        m.write(\"m.lp\")\n",
    "    \n",
    "        m.optimize()\n",
    "        times.append(m.Runtime)\n",
    "        objs.append(m.objVal)\n",
    "        num_binvars.append(m.NumBinVars)\n",
    "        num_constrs.append(m.NumConstrs)\n",
    "    \n",
    "    times_n[n] = times\n",
    "    objs_n[n] = objs\n",
    "    num_binvars_n[n] = num_binvars\n",
    "    num_constrs_n[n] = num_constrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40558b-c478-42a6-9f71-b4f61b43bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(times_n)\n",
    "print(objs_n)\n",
    "print(num_binvars_n)\n",
    "print(num_constrs_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b37526-3cd7-477d-a9a3-f11c5f5a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(path, file, filename):\n",
    "    file_loc = path + '/' + filename + '.pickle'\n",
    "    with open(file_loc, 'wb') as handle:\n",
    "        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# create the directory to save the results\n",
    "path = './results_opt_relu'\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except FileExistsError:\n",
    "    print('Folder already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e532f1-816f-41fa-806a-23440251371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(path, objs_n, 'relu_objs_n')\n",
    "pickle_save(path, times_n, 'relu_times_n')\n",
    "pickle_save(path, num_binvars_n, 'relu_binvars_n')\n",
    "pickle_save(path, num_constrs_n, 'relu_constrs_n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
