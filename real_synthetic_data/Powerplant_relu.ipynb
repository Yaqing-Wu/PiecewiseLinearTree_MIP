{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e6422-b044-4654-978f-66834202659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044297e-49c0-4ff9-8066-37742badf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4 # max dimention of the input vector\n",
    "n_samples = 10 # number of sampled quadratic function for each dimension\n",
    "\n",
    "# set random seed\n",
    "global_seed = 777\n",
    "random.seed(global_seed)\n",
    "np.random.seed(global_seed)\n",
    "seeds = np.random.randint(1,1000,(1,n_samples))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dea54-82d3-417c-8af1-dc57be0379f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [] # list that stores solution times\n",
    "objs = [] # list that stores obj function values\n",
    "num_binvars = [] # list that stores # binary variables\n",
    "num_constrs = [] # list that stores # constraints\n",
    "best_models = [] # list that stores the best trained model\n",
    "mapes = [] # list that stores MAPEs of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e1744-0bb9-47e6-a3bc-1450389b9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_samples)):\n",
    "    \n",
    "    # read data from csv file\n",
    "    file_name = './power_plant.csv'\n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    # Y = ReLU_Network(X)\n",
    "    X_variable_raw = df.iloc[:,:-1].values\n",
    "    Y_variable_raw = df.iloc[:,-1].values\n",
    "    X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_variable_raw, Y_variable_raw,\n",
    "                                                                        random_state=seeds[i], test_size = 0.25)\n",
    "    \n",
    "    # scaling\n",
    "    sc_X = MinMaxScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "\n",
    "    X_variable = sc_X.fit_transform(X_train_raw)\n",
    "    Y_variable = sc_y.fit_transform(y_train_raw.reshape(-1,1))\n",
    "    Y_variable = np.squeeze(Y_variable)\n",
    "    \n",
    "    labels=np.array(Y_variable)\n",
    "    features = np.array(X_variable)\n",
    "    \n",
    "    #training\n",
    "    random.seed(seeds[i])\n",
    "    np.random.seed(seeds[i])\n",
    "    gs_relu = GridSearchCV(MLPRegressor(max_iter = 1000),\n",
    "\t\t\t\t  param_grid = {'learning_rate_init': [1e-2, 2e-2, 5e-2, 1e-3, 2e-3, 5e-3, 1e-4],\n",
    "                                'hidden_layer_sizes': [(10, 5), (20, 10), (40, 20), (50, 20), (50, 30, 30)]},\n",
    "\t\t\t\t  return_train_score = True, cv = 5,\n",
    "\t\t\t\t  scoring = 'neg_mean_squared_error',\n",
    "                  verbose = 0)\n",
    "    gs_relu.fit(features, labels)\n",
    "    nn = gs_relu.best_estimator_\n",
    "    best_models.append(nn)\n",
    "    \n",
    "    # prediction accuracy \n",
    "    y_predict = nn.predict(sc_X.transform(X_test_raw))\n",
    "    y_predict_inversed = sc_y.inverse_transform(y_predict.reshape(-1,1))\n",
    "    mapes.append(mean_absolute_percentage_error(y_test_raw, y_predict_inversed))\n",
    "    \n",
    "    # obtain MILP parameters from the ReLU network structure/parameters\n",
    "    # weight and bias\n",
    "    weights = nn.coefs_\n",
    "    biases = nn.intercepts_\n",
    "    \n",
    "    # kj_raw: number of neurons in the ith hidden layer; k_hidden: number of hidden layers\n",
    "    kj_raw = nn.hidden_layer_sizes\n",
    "    k_hidden = nn.n_layers_ - 2\n",
    "    \n",
    "    # indices_k_hidden: set of hidden layers; indices_kj_hidden: set of tuple (layer k, jth neuron in this layer)\n",
    "    # indices_kj_dict: mapping of hidden layer k -> list of neurons j in this hidden layer\n",
    "    indices_k_hidden = range(1, k_hidden+1)\n",
    "    indices_kj_hidden = []\n",
    "    indices_kj_dict = defaultdict(list)\n",
    "    for k in indices_k_hidden:\n",
    "        for j in range(kj_raw[k-1]):\n",
    "            indices_kj_hidden.append((k,j))\n",
    "            indices_kj_dict[k].append(j)\n",
    "    indices_kj_dict[nn.n_layers_ - 1] = [0]\n",
    "    indices_kj_dict[0] = list(range(n))\n",
    "    # indices_kj_first: (k, j) tuple for input layer; indices_kj_last: (k, j) tuple for the output layer\n",
    "    # indices_kj: (k, j) tuple for all the layers in the network\n",
    "    # indices_kj_nofirst: (k, j) tuple for all the layers except for the first layer      \n",
    "    indices_kj_first = [(0,i) for i in range(n)]\n",
    "    indices_kj_last = [(nn.n_layers_ - 1, 0)]\n",
    "    indices_kj = indices_kj_first + indices_kj_hidden + indices_kj_last\n",
    "    indices_kj_nofirst = indices_kj_hidden + indices_kj_last\n",
    "    \n",
    "    # paramters\n",
    "    \n",
    "    # lower/upper bound of variable X\n",
    "    # x_lb_first/x_ub_first: lower/upper bound of variable X for the input layer\n",
    "    x_lb_first = np.amin(X_variable_raw, axis = 0).tolist()\n",
    "    x_ub_first = np.amax(X_variable_raw, axis = 0).tolist()\n",
    "    \n",
    "    # x_ub/x_lb: lower/upper bound of variable X in all the layers\n",
    "    # x_ub_dict: mapping of layer -> list of upper bound of all neurons in the layer\n",
    "    bigM = 1e4\n",
    "    x_ub = x_ub_first + [bigM] * len(indices_kj_hidden) + [bigM]\n",
    "    x_lb = x_lb_first + [0] * len(indices_kj_hidden) + [0]\n",
    "    x_ub_dict = defaultdict(list)\n",
    "    x_ub_dict[0] = x_ub_first\n",
    "    for k in indices_k_hidden:\n",
    "        x_ub_dict[k] = [bigM] * kj_raw[k-1]\n",
    "    x_ub_dict[k_hidden + 1] = [bigM]\n",
    "    \n",
    "    # scaling: the network is trained using the min-max scaled variables, so we need to have scaler_coeff to do\n",
    "    # (1) min_max scaling the input variable (happened at the input layer)\n",
    "    # (2) min_max scaling the output variable (happened at the output layer)\n",
    "    # to make the constraints consistent we still have scaler_coeff[k] for hidden layer but the values are set to 1 (no scaling)\n",
    "    scaler_coeff = {}\n",
    "    scaler_coeff[0] = np.reciprocal(sc_X.data_range_).tolist() # scaler_coeff = 1/(Xmax - Xmin) (i.e., the original range)\n",
    "    for k in indices_k_hidden:\n",
    "        scaler_coeff[k] = [1] * kj_raw[k-1] # scaler_coeff = 1 (no scaling)\n",
    "    scaler_coeff[k_hidden + 1] = np.reciprocal(sc_y.data_range_).tolist() # scaler_coeff = 1/(Ymax - Ymin)\n",
    "    \n",
    "    scaler_min = {}\n",
    "    scaler_min[0] = sc_X.data_min_.tolist() # Xmin before scaling\n",
    "    for k in indices_k_hidden:\n",
    "        scaler_min[k] = [0] * kj_raw[k-1] # Xmin = 0 (no scaling)\n",
    "    scaler_min[k_hidden + 1] = sc_y.data_min_.tolist() # Ymin before scaling\n",
    "    \n",
    "    # create a new model\n",
    "    m = gp.Model(\"RELU\")\n",
    "    m.Params.LogToConsole = 0\n",
    "    m.setParam('TimeLimit', 60)\n",
    "\n",
    "    # create variables\n",
    "    z = m.addVars(indices_kj_nofirst, name = 'z', vtype = GRB.BINARY)\n",
    "    x = m.addVars(indices_kj, ub = x_ub, lb = x_lb, name = 'x')\n",
    "    s = m.addVars(indices_kj_nofirst, name = 's')\n",
    "    \n",
    "    # maximization\n",
    "    m.setObjective(x[nn.n_layers_-1,0], GRB.MAXIMIZE)\n",
    "\n",
    "    # add constraint\n",
    "    m.addConstrs((gp.quicksum(weights[k-1][l][j] * (x[k-1,l] - scaler_min[k-1][l]) * scaler_coeff[k-1][l] for l in indices_kj_dict[k-1]) + biases[k-1][j] \\\n",
    "                  == (x[k,j] - scaler_min[k][j]) * scaler_coeff[k][j] - s[k,j] for (k,j) in indices_kj_nofirst),\n",
    "                 name = 'calc_hidden_layers')\n",
    "    m.addConstrs((x[k,j] <= x_ub_dict[k][j] * z[k,j] for (k,j) in indices_kj_nofirst), name = \"constraint_x\")\n",
    "    m.addConstrs((s[k,j] <= x_ub_dict[k][j] * (1 - z[k,j]) for (k,j) in indices_kj_nofirst), name = \"constraint_z\")\n",
    "    \n",
    "    m.update()\n",
    "    m.write(\"m.lp\")\n",
    "    \n",
    "    m.optimize()\n",
    "    \n",
    "    times.append(m.Runtime)\n",
    "    objs.append(m.objVal)\n",
    "    num_binvars.append(m.NumBinVars)\n",
    "    num_constrs.append(m.NumConstrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65454996-d017-44f3-b16c-15445a8f9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(times)\n",
    "print(objs)\n",
    "print(num_binvars)\n",
    "print(num_constrs)\n",
    "print(mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a11a8c-5e12-49f1-a038-663f82460d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(path, file, filename):\n",
    "    file_loc = path + '/' + filename + '.pickle'\n",
    "    with open(file_loc, 'wb') as handle:\n",
    "        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# create the directory to save the results\n",
    "path = './results_opt_powerplant_relu'\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except FileExistsError:\n",
    "    print('Folder already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fc713-3f73-4a56-adaa-1e5fe3310cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(path, objs, 'objs')\n",
    "pickle_save(path, times, 'times')\n",
    "pickle_save(path, num_binvars, 'num_binvars')\n",
    "pickle_save(path, num_constrs, 'num_constrs')\n",
    "pickle_save(path, mapes, 'mapes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
